{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2_Deep_Fakes_Detection_Project.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMTFIBie2EtRxTA0RNFDwvp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "5b87f4b5215c487ebe35278ca9f642b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_d8121da742894cefa6de0ba03ac2a7c7",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_76fa66fe13484ec6a4bde6f5816b6119",
              "IPY_MODEL_5971de8d8f8b4ac09aab1f24e35393e9"
            ]
          }
        },
        "d8121da742894cefa6de0ba03ac2a7c7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "76fa66fe13484ec6a4bde6f5816b6119": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_3c3c567153614d619d8c67f2d3b543e8",
            "_dom_classes": [],
            "description": "",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 46827520,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 46827520,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_50d3d4ffe319488881ea31b576403280"
          }
        },
        "5971de8d8f8b4ac09aab1f24e35393e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_41e7f7a73de045a0baa1ad7c40069d3a",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100% 44.7M/44.7M [00:00&lt;00:00, 52.8MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_68b59fe6301a468fa1680f98bb54f157"
          }
        },
        "3c3c567153614d619d8c67f2d3b543e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "50d3d4ffe319488881ea31b576403280": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "41e7f7a73de045a0baa1ad7c40069d3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "68b59fe6301a468fa1680f98bb54f157": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anubhavgupta1/Deep-Learning-Projects/blob/master/2_Deep_Fakes_Detection_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYGIy5_Wi-HR",
        "colab_type": "text"
      },
      "source": [
        "#Deep Fakes Detection Project"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "est9FNq1aoPR",
        "colab_type": "code",
        "outputId": "e16a4929-de67-4618-96ad-439cd8950b55",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "!ls\n",
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "from oauth2client.client import GoogleCredentials\n",
        "creds = GoogleCredentials.get_application_default()\n",
        "import getpass\n",
        "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "vcode = getpass.getpass()\n",
        "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}\n",
        "\n",
        "\n",
        "!mkdir -p drive\n",
        "!google-drive-ocamlfuse drive\n",
        "\n",
        "import os\n",
        "os.chdir(\"drive/Dataset/classification\")\n",
        "!ls"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sample_data\n",
            "E: Package 'python-software-properties' has no installation candidate\n",
            "Selecting previously unselected package google-drive-ocamlfuse.\n",
            "(Reading database ... 145113 files and directories currently installed.)\n",
            "Preparing to unpack .../google-drive-ocamlfuse_0.7.17-0ubuntu2~ubuntu18.04.1_amd64.deb ...\n",
            "Unpacking google-drive-ocamlfuse (0.7.17-0ubuntu2~ubuntu18.04.1) ...\n",
            "Setting up google-drive-ocamlfuse (0.7.17-0ubuntu2~ubuntu18.04.1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "··········\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "Please enter the verification code: Access token retrieved correctly.\n",
            "2_Deep_fakes_Detection_using_pyTorch.ipynb  drive   xception-b5690688.pth\n",
            "Dataset\t\t\t\t\t    output\n",
            "dataset.csv\t\t\t\t    tain\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eyyi-zXKbTIU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "videos_temp = []\n",
        "labels_temp = []\n",
        "X = []\n",
        "y = []\n",
        "\n",
        "with open('dataset.csv', 'r') as f: \n",
        "    for line in f:\n",
        "        line = line.split(',')\n",
        "        video_t = line[0].strip()\n",
        "        label_t = line[1].strip()\n",
        "        if not label_t in videos_temp:\n",
        "            videos_temp.append(label_t)\n",
        "        X.append(video_t )\n",
        "        y.append(label_t)\n",
        "        labels_temp.append((video_t , label_t))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eRP26IHdbWnG",
        "colab_type": "code",
        "outputId": "ef385cd5-73fb-489a-83a2-25c4b259b758",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        }
      },
      "source": [
        "print(len(labels_temp))\n",
        "print(*labels_temp[0:10], sep = \"\\n\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "400\n",
            "('a1', '0')\n",
            "('a2', '1')\n",
            "('a3', '1')\n",
            "('a4', '0')\n",
            "('a5', '0')\n",
            "('a6', '0')\n",
            "('a7', '1')\n",
            "('a8', '1')\n",
            "('a9', '0')\n",
            "('a10', '0')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UT7PXvXuYMAj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "actual_output={}\n",
        "for data in labels_temp:\n",
        "  actual_output[data[0]] = data[1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gl492fBhHjJW",
        "colab_type": "code",
        "outputId": "49d2a633-9423-4d85-90b1-ef3ba7ca2195",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        }
      },
      "source": [
        "!pip install pretrainedmodels"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pretrainedmodels\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/84/0e/be6a0e58447ac16c938799d49bfb5fb7a80ac35e137547fc6cee2c08c4cf/pretrainedmodels-0.7.4.tar.gz (58kB)\n",
            "\r\u001b[K     |█████▋                          | 10kB 16.8MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 20kB 2.2MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 30kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 40kB 2.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 51kB 2.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 61kB 2.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from pretrainedmodels) (1.4.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (from pretrainedmodels) (0.5.0)\n",
            "Collecting munch\n",
            "  Downloading https://files.pythonhosted.org/packages/cc/ab/85d8da5c9a45e072301beb37ad7f833cd344e04c817d97e0cc75681d248f/munch-2.5.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pretrainedmodels) (4.28.1)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision->pretrainedmodels) (6.2.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision->pretrainedmodels) (1.12.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision->pretrainedmodels) (1.17.5)\n",
            "Building wheels for collected packages: pretrainedmodels\n",
            "  Building wheel for pretrainedmodels (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pretrainedmodels: filename=pretrainedmodels-0.7.4-cp36-none-any.whl size=60962 sha256=41856630ede3ac685ab0c41d956fdcaf02f9dbbe0ad6a927bae15781fa067192\n",
            "  Stored in directory: /root/.cache/pip/wheels/69/df/63/62583c096289713f22db605aa2334de5b591d59861a02c2ecd\n",
            "Successfully built pretrainedmodels\n",
            "Installing collected packages: munch, pretrainedmodels\n",
            "Successfully installed munch-2.5.0 pretrainedmodels-0.7.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ul2eft-xJwom",
        "colab_type": "code",
        "outputId": "f42ce67e-ad26-4603-a1e7-d58a288f29b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!pwd"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/Dataset/classification\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bHDH04r8J3_i",
        "colab_type": "code",
        "outputId": "787dec3b-6845-40f7-9edc-2965cc660d5a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2_Deep_fakes_Detection_using_pyTorch.ipynb  drive   xception-b5690688.pth\n",
            "Dataset\t\t\t\t\t    output\n",
            "dataset.csv\t\t\t\t    tain\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yat9-UyIO-RS",
        "colab_type": "code",
        "outputId": "c51ef5a5-ef03-4f53-cf62-4369ea8be0dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "video_path = \"/content/drive/Dataset/classification/Dataset/\"\n",
        "videos = os.listdir(video_path)\n",
        "print(\"Total videos = \"+str(len(videos)))\n",
        "print(video_path + videos[0])\n",
        "\"\"\"\n",
        "for video in videos:\n",
        "  video_path_test = (video_path + video)\n",
        "  print(video_path_test)\n",
        "  \"\"\""
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total videos = 400\n",
            "/content/drive/Dataset/classification/Dataset/b64.mp4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nfor video in videos:\\n  video_path_test = (video_path + video)\\n  print(video_path_test)\\n  '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QEX9LynE5DkK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tqdm import tqdm\n",
        "import os\n",
        "import argparse\n",
        "from os.path import join\n",
        "\n",
        "\n",
        "import torch\n",
        "import pretrainedmodels\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "import torchvision\n",
        "\n",
        "import cv2\n",
        "import dlib\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "from torch.nn import init\n",
        "import torch.utils.model_zoo as model_zoo\n",
        "from PIL import Image as pil_image\n",
        "\n",
        "from torchsummary import summary\n",
        "from torchvision import transforms\n",
        "\n",
        "from torchvision import models"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CRvB0-fncFSU",
        "colab_type": "code",
        "outputId": "33f51496-7043-48e7-d31f-3f4b41893897",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kQvpsBMDk-IM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "xception_default_data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.Resize((299, 299)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.5]*3, [0.5]*3)\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "        transforms.Resize((299, 299)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.5] * 3, [0.5] * 3)\n",
        "    ]),\n",
        "    'test': transforms.Compose([\n",
        "        transforms.Resize((299, 299)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.5] * 3, [0.5] * 3)\n",
        "    ]),\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FGcCejaem7hw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pretrained_settings = {\n",
        "    'xception': {\n",
        "        'imagenet': {\n",
        "            'url': 'http://data.lip6.fr/cadene/pretrainedmodels/xception-b5690688.pth',\n",
        "            'input_space': 'RGB',\n",
        "            'input_size': [3, 299, 299],\n",
        "            'input_range': [0, 1],\n",
        "            'mean': [0.5, 0.5, 0.5],\n",
        "            'std': [0.5, 0.5, 0.5],\n",
        "            'num_classes': 1000,\n",
        "            'scale': 0.8975 # The resize parameter of the validation transform should be 333, and make sure to center crop at 299x299\n",
        "        }\n",
        "    }\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cmp43ivYpxLA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SeparableConv2d(nn.Module):\n",
        "    def __init__(self,in_channels,out_channels,kernel_size=1,stride=1,padding=0,dilation=1,bias=False):\n",
        "        super(SeparableConv2d,self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels,in_channels,kernel_size,stride,padding,dilation,groups=in_channels,bias=bias)\n",
        "        self.pointwise = nn.Conv2d(in_channels,out_channels,1,1,0,1,1,bias=bias)\n",
        "\n",
        "    def forward(self,x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.pointwise(x)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wpa5NDZ2pgst",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Block(nn.Module):\n",
        "    def __init__(self,in_filters,out_filters,reps,strides=1,start_with_relu=True,grow_first=True):\n",
        "        super(Block, self).__init__()\n",
        "\n",
        "        if out_filters != in_filters or strides!=1:\n",
        "            self.skip = nn.Conv2d(in_filters,out_filters,1,stride=strides, bias=False)\n",
        "            self.skipbn = nn.BatchNorm2d(out_filters)\n",
        "        else:\n",
        "            self.skip=None\n",
        "\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        rep=[]\n",
        "\n",
        "        filters=in_filters\n",
        "        if grow_first:\n",
        "            rep.append(self.relu)\n",
        "            rep.append(SeparableConv2d(in_filters,out_filters,3,stride=1,padding=1,bias=False))\n",
        "            rep.append(nn.BatchNorm2d(out_filters))\n",
        "            filters = out_filters\n",
        "\n",
        "        for i in range(reps-1):\n",
        "            rep.append(self.relu)\n",
        "            rep.append(SeparableConv2d(filters,filters,3,stride=1,padding=1,bias=False))\n",
        "            rep.append(nn.BatchNorm2d(filters))\n",
        "\n",
        "        if not grow_first:\n",
        "            rep.append(self.relu)\n",
        "            rep.append(SeparableConv2d(in_filters,out_filters,3,stride=1,padding=1,bias=False))\n",
        "            rep.append(nn.BatchNorm2d(out_filters))\n",
        "\n",
        "        if not start_with_relu:\n",
        "            rep = rep[1:]\n",
        "        else:\n",
        "            rep[0] = nn.ReLU(inplace=False)\n",
        "\n",
        "        if strides != 1:\n",
        "            rep.append(nn.MaxPool2d(3,strides,1))\n",
        "        self.rep = nn.Sequential(*rep)\n",
        "\n",
        "    def forward(self,inp):\n",
        "        x = self.rep(inp)\n",
        "\n",
        "        if self.skip is not None:\n",
        "            skip = self.skip(inp)\n",
        "            skip = self.skipbn(skip)\n",
        "        else:\n",
        "            skip = inp\n",
        "\n",
        "        x+=skip\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JG9ttDq_prFd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Xception(nn.Module):\n",
        "    \"\"\"\n",
        "    Xception optimized for the ImageNet dataset, as specified in\n",
        "    https://arxiv.org/pdf/1610.02357.pdf\n",
        "    \"\"\"\n",
        "    def __init__(self, num_classes=1000):\n",
        "        \"\"\" Constructor\n",
        "        Args:\n",
        "            num_classes: number of classes\n",
        "        \"\"\"\n",
        "        super(Xception, self).__init__()\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 32, 3,2, 0, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(32,64,3,bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        #do relu here\n",
        "\n",
        "        self.block1=Block(64,128,2,2,start_with_relu=False,grow_first=True)\n",
        "        self.block2=Block(128,256,2,2,start_with_relu=True,grow_first=True)\n",
        "        self.block3=Block(256,728,2,2,start_with_relu=True,grow_first=True)\n",
        "\n",
        "        self.block4=Block(728,728,3,1,start_with_relu=True,grow_first=True)\n",
        "        self.block5=Block(728,728,3,1,start_with_relu=True,grow_first=True)\n",
        "        self.block6=Block(728,728,3,1,start_with_relu=True,grow_first=True)\n",
        "        self.block7=Block(728,728,3,1,start_with_relu=True,grow_first=True)\n",
        "\n",
        "        self.block8=Block(728,728,3,1,start_with_relu=True,grow_first=True)\n",
        "        self.block9=Block(728,728,3,1,start_with_relu=True,grow_first=True)\n",
        "        self.block10=Block(728,728,3,1,start_with_relu=True,grow_first=True)\n",
        "        self.block11=Block(728,728,3,1,start_with_relu=True,grow_first=True)\n",
        "\n",
        "        self.block12=Block(728,1024,2,2,start_with_relu=True,grow_first=False)\n",
        "\n",
        "        self.conv3 = SeparableConv2d(1024,1536,3,1,1)\n",
        "        self.bn3 = nn.BatchNorm2d(1536)\n",
        "\n",
        "        #do relu here\n",
        "        self.conv4 = SeparableConv2d(1536,2048,3,1,1)\n",
        "        self.bn4 = nn.BatchNorm2d(2048)\n",
        "\n",
        "        self.fc = nn.Linear(2048, num_classes)\n",
        "\n",
        "        # #------- init weights --------\n",
        "        # for m in self.modules():\n",
        "        #     if isinstance(m, nn.Conv2d):\n",
        "        #         n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "        #         m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "        #     elif isinstance(m, nn.BatchNorm2d):\n",
        "        #         m.weight.data.fill_(1)\n",
        "        #         m.bias.data.zero_()\n",
        "        # #-----------------------------\n",
        "\n",
        "    def features(self, input):\n",
        "        x = self.conv1(input)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = self.block1(x)\n",
        "        x = self.block2(x)\n",
        "        x = self.block3(x)\n",
        "        x = self.block4(x)\n",
        "        x = self.block5(x)\n",
        "        x = self.block6(x)\n",
        "        x = self.block7(x)\n",
        "        x = self.block8(x)\n",
        "        x = self.block9(x)\n",
        "        x = self.block10(x)\n",
        "        x = self.block11(x)\n",
        "        x = self.block12(x)\n",
        "\n",
        "        x = self.conv3(x)\n",
        "        x = self.bn3(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = self.conv4(x)\n",
        "        x = self.bn4(x)\n",
        "        return x\n",
        "\n",
        "    def logits(self, features):\n",
        "        x = self.relu(features)\n",
        "\n",
        "        x = F.adaptive_avg_pool2d(x, (1, 1))\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.last_linear(x)\n",
        "        return x\n",
        "\n",
        "    def forward(self, input):\n",
        "        x = self.features(input)\n",
        "        x = self.logits(x)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WOfwxqDTp6vk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def xception(num_classes=1000, pretrained='imagenet'):\n",
        "    model = Xception(num_classes=num_classes)\n",
        "    if pretrained:\n",
        "        settings = pretrained_settings['xception'][pretrained]\n",
        "        assert num_classes == settings['num_classes'], \\\n",
        "            \"num_classes should be {}, but is {}\".format(settings['num_classes'], num_classes)\n",
        "\n",
        "        model = Xception(num_classes=num_classes)\n",
        "        model.load_state_dict(model_zoo.load_url(settings['url']))\n",
        "\n",
        "        model.input_space = settings['input_space']\n",
        "        model.input_size = settings['input_size']\n",
        "        model.input_range = settings['input_range']\n",
        "        model.mean = settings['mean']\n",
        "        model.std = settings['std']\n",
        "\n",
        "   \n",
        "    model.last_linear = model.fc\n",
        "    del model.fc\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oxhV9YIXE_3t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def return_pytorch04_xception(pretrained=True):\n",
        "    # Raises warning \"src not broadcastable to dst\" but thats fine\n",
        "    model = xception(pretrained=False)\n",
        "    if pretrained:\n",
        "        # Load model in torch 0.4+\n",
        "        model.fc = model.last_linear\n",
        "        del model.last_linear\n",
        "        state_dict = torch.load(F\"/content/drive/Dataset/classification/xception-b5690688.pth\")\n",
        "        for name, weights in state_dict.items():\n",
        "            if 'pointwise' in name:\n",
        "                state_dict[name] = weights.unsqueeze(-1).unsqueeze(-1)\n",
        "        model.load_state_dict(state_dict)\n",
        "        model.last_linear = model.fc\n",
        "        del model.fc\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ubZXJOvGFN3T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TransferModel(nn.Module):\n",
        "    \"\"\"\n",
        "    Simple transfer learning model that takes an imagenet pretrained model with\n",
        "    a fc layer as base model and retrains a new fc layer for num_out_classes\n",
        "    \"\"\"\n",
        "    def __init__(self, modelchoice, num_out_classes=2, dropout=0.0):\n",
        "        super(TransferModel, self).__init__()\n",
        "        self.modelchoice = modelchoice\n",
        "        if modelchoice == 'xception':\n",
        "            self.model = return_pytorch04_xception()\n",
        "            # Replace fc\n",
        "            num_ftrs = self.model.last_linear.in_features\n",
        "            if not dropout:\n",
        "                self.model.last_linear = nn.Linear(num_ftrs, num_out_classes)\n",
        "            else:\n",
        "                print('Using dropout', dropout)\n",
        "                self.model.last_linear = nn.Sequential(\n",
        "                    nn.Dropout(p=dropout),\n",
        "                    nn.Linear(num_ftrs, num_out_classes)\n",
        "                )\n",
        "        elif modelchoice == 'resnet50' or modelchoice == 'resnet18':\n",
        "            if modelchoice == 'resnet50':\n",
        "                self.model = models.resnet50(pretrained=True)\n",
        "            if modelchoice == 'resnet18':\n",
        "                self.model = models.resnet18(pretrained=True)\n",
        "            # Replace fc\n",
        "            num_ftrs = self.model.fc.in_features\n",
        "            if not dropout:\n",
        "                self.model.fc = nn.Linear(num_ftrs, num_out_classes)\n",
        "            else:\n",
        "                self.model.fc = nn.Sequential(\n",
        "                    nn.Dropout(p=dropout),\n",
        "                    nn.Linear(num_ftrs, num_out_classes)\n",
        "                )\n",
        "        else:\n",
        "            raise Exception('Choose valid model, e.g. resnet50')\n",
        "\n",
        "    def set_trainable_up_to(self, boolean, layername=\"Conv2d_4a_3x3\"):\n",
        "        \"\"\"\n",
        "        Freezes all layers below a specific layer and sets the following layers\n",
        "        to true if boolean else only the fully connected final layer\n",
        "        :param boolean:\n",
        "        :param layername: depends on network, for inception e.g. Conv2d_4a_3x3\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        # Stage-1: freeze all the layers\n",
        "        if layername is None:\n",
        "            for i, param in self.model.named_parameters():\n",
        "                param.requires_grad = True\n",
        "                return\n",
        "        else:\n",
        "            for i, param in self.model.named_parameters():\n",
        "                param.requires_grad = False\n",
        "        if boolean:\n",
        "            # Make all layers following the layername layer trainable\n",
        "            ct = []\n",
        "            found = False\n",
        "            for name, child in self.model.named_children():\n",
        "                if layername in ct:\n",
        "                    found = True\n",
        "                    for params in child.parameters():\n",
        "                        params.requires_grad = True\n",
        "                ct.append(name)\n",
        "            if not found:\n",
        "                raise Exception('Layer not found, cant finetune!'.format(\n",
        "                    layername))\n",
        "        else:\n",
        "            if self.modelchoice == 'xception':\n",
        "                # Make fc trainable\n",
        "                for param in self.model.last_linear.parameters():\n",
        "                    param.requires_grad = True\n",
        "\n",
        "            else:\n",
        "                # Make fc trainable\n",
        "                for param in self.model.fc.parameters():\n",
        "                    param.requires_grad = True\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.model(x)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQaqHhtXFRWb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model_selection(modelname, num_out_classes,\n",
        "                    dropout=None):\n",
        "    \"\"\"\n",
        "    :param modelname:\n",
        "    :return: model, image size, pretraining<yes/no>, input_list\n",
        "    \"\"\"\n",
        "    if modelname == 'xception':\n",
        "        return TransferModel(modelchoice='xception',\n",
        "                             num_out_classes=num_out_classes), 299, \\\n",
        "               True, ['image'], None\n",
        "    elif modelname == 'resnet18':\n",
        "        return TransferModel(modelchoice='resnet18', dropout=dropout,\n",
        "                             num_out_classes=num_out_classes), \\\n",
        "               224, True, ['image'], None\n",
        "    else:\n",
        "        raise NotImplementedError(modelname)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_f_UILFCgMZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_boundingbox(face, width, height, scale=1.3, minsize=None):\n",
        "    \"\"\"\n",
        "    Expects a dlib face to generate a quadratic bounding box.\n",
        "    :param face: dlib face class\n",
        "    :param width: frame width\n",
        "    :param height: frame height\n",
        "    :param scale: bounding box size multiplier to get a bigger face region\n",
        "    :param minsize: set minimum bounding box size\n",
        "    :return: x, y, bounding_box_size in opencv form\n",
        "    \"\"\"\n",
        "    x1 = face.left()\n",
        "    y1 = face.top()\n",
        "    x2 = face.right()\n",
        "    y2 = face.bottom()\n",
        "    size_bb = int(max(x2 - x1, y2 - y1) * scale)\n",
        "    if minsize:\n",
        "        if size_bb < minsize:\n",
        "            size_bb = minsize\n",
        "    center_x, center_y = (x1 + x2) // 2, (y1 + y2) // 2\n",
        "\n",
        "    # Check for out of bounds, x-y top left corner\n",
        "    x1 = max(int(center_x - size_bb // 2), 0)\n",
        "    y1 = max(int(center_y - size_bb // 2), 0)\n",
        "    # Check for too big bb size for given x, y\n",
        "    size_bb = min(width - x1, size_bb)\n",
        "    size_bb = min(height - y1, size_bb)\n",
        "\n",
        "    return x1, y1, size_bb"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_dhRt60CikD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess_image(image, cuda=True):\n",
        "    \"\"\"\n",
        "    Preprocesses the image such that it can be fed into our network.\n",
        "    During this process we envoke PIL to cast it into a PIL image.\n",
        "    :param image: numpy image in opencv form (i.e., BGR and of shape\n",
        "    :return: pytorch tensor of shape [1, 3, image_size, image_size], not\n",
        "    necessarily casted to cuda\n",
        "    \"\"\"\n",
        "    # Revert from BGR\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    # Preprocess using the preprocessing function used during training and\n",
        "    # casting it to PIL image\n",
        "    preprocess = xception_default_data_transforms['test']\n",
        "    preprocessed_image = preprocess(pil_image.fromarray(image))\n",
        "    # Add first dimension as the network expects a batch\n",
        "    preprocessed_image = preprocessed_image.unsqueeze(0)\n",
        "    if cuda:\n",
        "        preprocessed_image = preprocessed_image.cuda()\n",
        "    return preprocessed_image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56DhCdvXCzgL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict_with_model(image, model, post_function=nn.Softmax(dim=1),cuda=True):\n",
        "    \"\"\"\n",
        "    Predicts the label of an input image. Preprocesses the input image and\n",
        "    casts it to cuda if required\n",
        "    :param image: numpy image\n",
        "    :param model: torch model with linear layer at the end\n",
        "    :param post_function: e.g., softmax\n",
        "    :param cuda: enables cuda, must be the same parameter as the model\n",
        "    :return: prediction (1 = fake, 0 = real)\n",
        "    \"\"\"\n",
        "    # Preprocess\n",
        "    preprocessed_image = preprocess_image(image, cuda)\n",
        "\n",
        "    # Model prediction\n",
        "    output = model(preprocessed_image)\n",
        "    output = post_function(output)\n",
        "\n",
        "    # Cast to desired\n",
        "    _, prediction = torch.max(output, 1)    # argmax\n",
        "    prediction = float(prediction.cpu().numpy())\n",
        "\n",
        "    return int(prediction), output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3KkM3smrC3ha",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_full_image_network(video_path, output_path,start_frame=0, end_frame=None, cuda=True):\n",
        "    \"\"\"\n",
        "    Reads a video and evaluates a subset of frames with the a detection network\n",
        "    that takes in a full frame. Outputs are only given if a face is present\n",
        "    and the face is highlighted using dlib.\n",
        "    :param video_path: path to video file\n",
        "    :param model_path: path to model file (should expect the full sized image)\n",
        "    :param output_path: path where the output video is stored\n",
        "    :param start_frame: first frame to evaluate\n",
        "    :param end_frame: last frame to evaluate\n",
        "    :param cuda: enable cuda\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    #print('Starting: {}'.format(video_path))\n",
        "    label_video = []\n",
        "\n",
        "    # Read and write\n",
        "    reader = cv2.VideoCapture(video_path)\n",
        "\n",
        "    video_fn = video_path.split('/')[-1].split('.')[0]+'.avi'\n",
        "    os.makedirs(output_path, exist_ok=True)\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'MJPG')\n",
        "    fps = reader.get(cv2.CAP_PROP_FPS)\n",
        "    num_frames = int(reader.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    writer = None\n",
        "\n",
        "    # Face detector\n",
        "    face_detector = dlib.get_frontal_face_detector()\n",
        "\n",
        "    # Load model\n",
        "    model, *_ = model_selection(modelname='xception', num_out_classes=2)\n",
        "    #print(\"Model : \")\n",
        "    #print(model)\n",
        "    model = model.cuda()\n",
        "    input_s = (3, image_size, image_size)\n",
        "    #print(summary(model, input_s))\n",
        "    \n",
        "    \n",
        "\n",
        "\n",
        "    # Text variables\n",
        "    font_face = cv2.FONT_HERSHEY_SIMPLEX\n",
        "    thickness = 2\n",
        "    font_scale = 1\n",
        "\n",
        "    # Frame numbers and length of output video\n",
        "    frame_num = 0\n",
        "    assert start_frame < num_frames - 1\n",
        "    end_frame = end_frame if end_frame else num_frames\n",
        "    pbar = tqdm(total=end_frame-start_frame)\n",
        "\n",
        "    while reader.isOpened():\n",
        "        _, image = reader.read()\n",
        "        if image is None:\n",
        "            break\n",
        "        frame_num += 1\n",
        "\n",
        "        if frame_num < start_frame:\n",
        "            continue\n",
        "        pbar.update(1)\n",
        "\n",
        "        # Image size\n",
        "        height, width = image.shape[:2]\n",
        "\n",
        "        # Init output writer\n",
        "        if writer is None:\n",
        "            writer = cv2.VideoWriter(join(output_path, video_fn), fourcc, fps,(height, width)[::-1])\n",
        "\n",
        "        # 2. Detect with dlib\n",
        "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "        faces = face_detector(gray, 1)\n",
        "        if len(faces):\n",
        "            # For now only take biggest face\n",
        "            face = faces[0]\n",
        "\n",
        "            # --- Prediction ---------------------------------------------------\n",
        "            # Face crop with dlib and bounding box scale enlargement\n",
        "            x, y, size = get_boundingbox(face, width, height)\n",
        "            cropped_face = image[y:y+size, x:x+size]\n",
        "\n",
        "            # Actual prediction using our model\n",
        "            prediction, output = predict_with_model(cropped_face, model,cuda=cuda)\n",
        "            # ------------------------------------------------------------------\n",
        "            label_video.append(prediction)\n",
        "\n",
        "            # Text and bb\n",
        "            x = face.left()\n",
        "            y = face.top()\n",
        "            w = face.right() - x\n",
        "            h = face.bottom() - y\n",
        "            label = 'fake' if prediction == 1 else 'real'\n",
        "            color = (0, 255, 0) if prediction == 0 else (0, 0, 255)\n",
        "            output_list = ['{0:.2f}'.format(float(x)) for x in\n",
        "                           output.detach().cpu().numpy()[0]]\n",
        "            cv2.putText(image, str(output_list)+'=>'+label, (x, y+h+30),\n",
        "                        font_face, font_scale,\n",
        "                        color, thickness, 2)\n",
        "            # draw box over face\n",
        "            cv2.rectangle(image, (x, y), (x + w, y + h), color, 2)\n",
        "\n",
        "        if frame_num >= end_frame:\n",
        "            break\n",
        "\n",
        "        # Show\n",
        "        #cv2.imshow('test', image)\n",
        "        #cv2_imshow(image)\n",
        "        cv2.waitKey(33)     # About 30 fps\n",
        "        writer.write(image)\n",
        "    pbar.close()\n",
        "    if writer is not None:\n",
        "        writer.release()\n",
        "        print('Finished! Output saved under {}'.format(output_path))\n",
        "    else:\n",
        "        print('Input video file was empty')\n",
        "    return 0 if label_video.count(0)>label_video.count(1) else 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kuSC4Bi5C_11",
        "colab_type": "code",
        "outputId": "4217e5d1-2c88-4c98-de26-40a07336e7b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "5b87f4b5215c487ebe35278ca9f642b7",
            "d8121da742894cefa6de0ba03ac2a7c7",
            "76fa66fe13484ec6a4bde6f5816b6119",
            "5971de8d8f8b4ac09aab1f24e35393e9",
            "3c3c567153614d619d8c67f2d3b543e8",
            "50d3d4ffe319488881ea31b576403280",
            "41e7f7a73de045a0baa1ad7c40069d3a",
            "68b59fe6301a468fa1680f98bb54f157"
          ]
        }
      },
      "source": [
        "model, image_size, *_ = model_selection('resnet18', num_out_classes=2)\n",
        "print(\"Model : Resenet18\")\n",
        "print(model)\n",
        "model = model.cuda()\n",
        "input_s = (3, image_size, image_size)\n",
        "print(summary(model, input_s))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to /root/.cache/torch/checkpoints/resnet18-5c106cde.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5b87f4b5215c487ebe35278ca9f642b7",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, max=46827520), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Model : Resenet18\n",
            "TransferModel(\n",
            "  (model): ResNet(\n",
            "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu): ReLU(inplace=True)\n",
            "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "    (layer1): Sequential(\n",
            "      (0): BasicBlock(\n",
            "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (1): BasicBlock(\n",
            "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (layer2): Sequential(\n",
            "      (0): BasicBlock(\n",
            "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (downsample): Sequential(\n",
            "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (1): BasicBlock(\n",
            "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (layer3): Sequential(\n",
            "      (0): BasicBlock(\n",
            "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (downsample): Sequential(\n",
            "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (1): BasicBlock(\n",
            "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (layer4): Sequential(\n",
            "      (0): BasicBlock(\n",
            "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (downsample): Sequential(\n",
            "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (1): BasicBlock(\n",
            "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "    (fc): Linear(in_features=512, out_features=2, bias=True)\n",
            "  )\n",
            ")\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 64, 112, 112]           9,408\n",
            "       BatchNorm2d-2         [-1, 64, 112, 112]             128\n",
            "              ReLU-3         [-1, 64, 112, 112]               0\n",
            "         MaxPool2d-4           [-1, 64, 56, 56]               0\n",
            "            Conv2d-5           [-1, 64, 56, 56]          36,864\n",
            "       BatchNorm2d-6           [-1, 64, 56, 56]             128\n",
            "              ReLU-7           [-1, 64, 56, 56]               0\n",
            "            Conv2d-8           [-1, 64, 56, 56]          36,864\n",
            "       BatchNorm2d-9           [-1, 64, 56, 56]             128\n",
            "             ReLU-10           [-1, 64, 56, 56]               0\n",
            "       BasicBlock-11           [-1, 64, 56, 56]               0\n",
            "           Conv2d-12           [-1, 64, 56, 56]          36,864\n",
            "      BatchNorm2d-13           [-1, 64, 56, 56]             128\n",
            "             ReLU-14           [-1, 64, 56, 56]               0\n",
            "           Conv2d-15           [-1, 64, 56, 56]          36,864\n",
            "      BatchNorm2d-16           [-1, 64, 56, 56]             128\n",
            "             ReLU-17           [-1, 64, 56, 56]               0\n",
            "       BasicBlock-18           [-1, 64, 56, 56]               0\n",
            "           Conv2d-19          [-1, 128, 28, 28]          73,728\n",
            "      BatchNorm2d-20          [-1, 128, 28, 28]             256\n",
            "             ReLU-21          [-1, 128, 28, 28]               0\n",
            "           Conv2d-22          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-23          [-1, 128, 28, 28]             256\n",
            "           Conv2d-24          [-1, 128, 28, 28]           8,192\n",
            "      BatchNorm2d-25          [-1, 128, 28, 28]             256\n",
            "             ReLU-26          [-1, 128, 28, 28]               0\n",
            "       BasicBlock-27          [-1, 128, 28, 28]               0\n",
            "           Conv2d-28          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-29          [-1, 128, 28, 28]             256\n",
            "             ReLU-30          [-1, 128, 28, 28]               0\n",
            "           Conv2d-31          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-32          [-1, 128, 28, 28]             256\n",
            "             ReLU-33          [-1, 128, 28, 28]               0\n",
            "       BasicBlock-34          [-1, 128, 28, 28]               0\n",
            "           Conv2d-35          [-1, 256, 14, 14]         294,912\n",
            "      BatchNorm2d-36          [-1, 256, 14, 14]             512\n",
            "             ReLU-37          [-1, 256, 14, 14]               0\n",
            "           Conv2d-38          [-1, 256, 14, 14]         589,824\n",
            "      BatchNorm2d-39          [-1, 256, 14, 14]             512\n",
            "           Conv2d-40          [-1, 256, 14, 14]          32,768\n",
            "      BatchNorm2d-41          [-1, 256, 14, 14]             512\n",
            "             ReLU-42          [-1, 256, 14, 14]               0\n",
            "       BasicBlock-43          [-1, 256, 14, 14]               0\n",
            "           Conv2d-44          [-1, 256, 14, 14]         589,824\n",
            "      BatchNorm2d-45          [-1, 256, 14, 14]             512\n",
            "             ReLU-46          [-1, 256, 14, 14]               0\n",
            "           Conv2d-47          [-1, 256, 14, 14]         589,824\n",
            "      BatchNorm2d-48          [-1, 256, 14, 14]             512\n",
            "             ReLU-49          [-1, 256, 14, 14]               0\n",
            "       BasicBlock-50          [-1, 256, 14, 14]               0\n",
            "           Conv2d-51            [-1, 512, 7, 7]       1,179,648\n",
            "      BatchNorm2d-52            [-1, 512, 7, 7]           1,024\n",
            "             ReLU-53            [-1, 512, 7, 7]               0\n",
            "           Conv2d-54            [-1, 512, 7, 7]       2,359,296\n",
            "      BatchNorm2d-55            [-1, 512, 7, 7]           1,024\n",
            "           Conv2d-56            [-1, 512, 7, 7]         131,072\n",
            "      BatchNorm2d-57            [-1, 512, 7, 7]           1,024\n",
            "             ReLU-58            [-1, 512, 7, 7]               0\n",
            "       BasicBlock-59            [-1, 512, 7, 7]               0\n",
            "           Conv2d-60            [-1, 512, 7, 7]       2,359,296\n",
            "      BatchNorm2d-61            [-1, 512, 7, 7]           1,024\n",
            "             ReLU-62            [-1, 512, 7, 7]               0\n",
            "           Conv2d-63            [-1, 512, 7, 7]       2,359,296\n",
            "      BatchNorm2d-64            [-1, 512, 7, 7]           1,024\n",
            "             ReLU-65            [-1, 512, 7, 7]               0\n",
            "       BasicBlock-66            [-1, 512, 7, 7]               0\n",
            "AdaptiveAvgPool2d-67            [-1, 512, 1, 1]               0\n",
            "           Linear-68                    [-1, 2]           1,026\n",
            "           ResNet-69                    [-1, 2]               0\n",
            "================================================================\n",
            "Total params: 11,177,538\n",
            "Trainable params: 11,177,538\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.57\n",
            "Forward/backward pass size (MB): 62.79\n",
            "Params size (MB): 42.64\n",
            "Estimated Total Size (MB): 106.00\n",
            "----------------------------------------------------------------\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZDcy6XJcrvA4",
        "colab_type": "code",
        "outputId": "245d29ff-2c1e-419b-c984-e7ad85c9dc9f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "source": [
        "%%time\n",
        "video_path = \"/content/drive/Dataset/classification/Dataset/\"\n",
        "output_path = \"/content/drive/Dataset/classification/output/\"\n",
        "videos = os.listdir(video_path)\n",
        "out_dict={}\n",
        "temp_videos=[]\n",
        "temp_videos.append(videos[0])\n",
        "temp_videos.append(videos[1])\n",
        "temp_videos.append(videos[2])\n",
        "temp_videos.append(videos[3])\n",
        "temp_videos.append(videos[4])\n",
        "print(temp_videos)\n",
        "for video in temp_videos:\n",
        "  video_path_test = (video_path + video)\n",
        "  out_dict[video] = test_full_image_network(video_path_test,output_path)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['a100.avi', 'a233.mp4', 'c20.avi', 'a27.mp4', 'c21.mp4']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1263/1263 [11:44<00:00,  1.90it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Finished! Output saved under /content/drive/Dataset/classification/output/\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 300/300 [05:44<00:00,  1.16s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Finished! Output saved under /content/drive/Dataset/classification/output/\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 446/446 [01:36<00:00,  4.72it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Finished! Output saved under /content/drive/Dataset/classification/output/\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 300/300 [05:45<00:00,  1.11s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Finished! Output saved under /content/drive/Dataset/classification/output/\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 300/300 [05:57<00:00,  1.11s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Finished! Output saved under /content/drive/Dataset/classification/output/\n",
            "CPU times: user 30min 57s, sys: 25.2 s, total: 31min 23s\n",
            "Wall time: 31min 29s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93bEwwt46q6o",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "18514643-ee71-4421-f924-69bf1586ca63"
      },
      "source": [
        "print(\"actual output : \"+str(actual_output['a100'])+\" Predicted Output : \" + str(out_dict['a100.avi']))\n",
        "print(\"actual output : \"+str(actual_output['a233'])+\" Predicted Output : \" + str(out_dict['a233.mp4']))\n",
        "print(\"actual output : \"+str(actual_output['c20'])+\" Predicted Output : \" + str(out_dict['c20.avi']))\n",
        "print(\"actual output : \"+str(actual_output['a27'])+\" Predicted Output : \" + str(out_dict['a27.mp4']))\n",
        "print(\"actual output : \"+str(actual_output['c21'])+\" Predicted Output : \" + str(out_dict['c21.mp4']))"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "actual output : 1 Predicted Output : 0\n",
            "actual output : 0 Predicted Output : 0\n",
            "actual output : 1 Predicted Output : 0\n",
            "actual output : 0 Predicted Output : 1\n",
            "actual output : 0 Predicted Output : 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "29IVaFAx1jqS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bcf06487-6bc5-4bdd-8b2e-9984fbbacafe"
      },
      "source": [
        "len(out_dict)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YlypVAJt1p1u",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "a8fece5d-1177-4c4c-e5c4-2eb413242e9e"
      },
      "source": [
        "correct_predictions =  0\n",
        "for keys in out_dict:\n",
        "  temp_keys = keys.split('.')[0]\n",
        "  print(\"actual output : \"+str(actual_output[temp_keys])+\" Predicted Output : \" + str(out_dict[keys]))\n"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "actual output : 1 Predicted Output : 0\n",
            "actual output : 0 Predicted Output : 0\n",
            "actual output : 1 Predicted Output : 0\n",
            "actual output : 0 Predicted Output : 1\n",
            "actual output : 0 Predicted Output : 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NkBxRMef5OFV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4ccd58aa-c65c-47d0-a003-4ab2fddded39"
      },
      "source": [
        "(correct_predictions/len(out_dict))*100"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    }
  ]
}