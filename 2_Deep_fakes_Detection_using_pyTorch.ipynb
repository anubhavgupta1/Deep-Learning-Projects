{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2_Deep_fakes_Detection_using_pyTorch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNROKjfIh8h1hx16gthRTH/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anubhavgupta1/Deep-Learning-Projects/blob/master/2_Deep_fakes_Detection_using_pyTorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HyedQSHGCkqT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "88bf2800-e552-48fb-d982-abce0891bac2"
      },
      "source": [
        "# RUN THIS ONLY IF YOU ARE IN GOOGLE COLAB NOTEBOOK\n",
        "!ls\n",
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "from oauth2client.client import GoogleCredentials\n",
        "creds = GoogleCredentials.get_application_default()\n",
        "import getpass\n",
        "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "vcode = getpass.getpass()\n",
        "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}\n",
        "\n",
        "\n",
        "!mkdir -p drive\n",
        "!google-drive-ocamlfuse drive\n",
        "\n",
        "import os\n",
        "os.chdir(\"drive/Dataset/classification\")\n",
        "!ls"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sample_data\n",
            "E: Package 'python-software-properties' has no installation candidate\n",
            "Selecting previously unselected package google-drive-ocamlfuse.\n",
            "(Reading database ... 145113 files and directories currently installed.)\n",
            "Preparing to unpack .../google-drive-ocamlfuse_0.7.17-0ubuntu2~ubuntu18.04.1_amd64.deb ...\n",
            "Unpacking google-drive-ocamlfuse (0.7.17-0ubuntu2~ubuntu18.04.1) ...\n",
            "Setting up google-drive-ocamlfuse (0.7.17-0ubuntu2~ubuntu18.04.1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "··········\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "Please enter the verification code: Access token retrieved correctly.\n",
            "dataset\t\t      out\t\ttain   validate\n",
            "detect_from_video.py  README.md\t\ttest   xception-b5690688.pth\n",
            "network\t\t      requirements.txt\ttrain\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gl492fBhHjJW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        },
        "outputId": "0cc7b3ce-9df7-4ecd-fcd2-4d3d15ac67c9"
      },
      "source": [
        "!pip install pretrainedmodels"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pretrainedmodels\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/84/0e/be6a0e58447ac16c938799d49bfb5fb7a80ac35e137547fc6cee2c08c4cf/pretrainedmodels-0.7.4.tar.gz (58kB)\n",
            "\r\u001b[K     |█████▋                          | 10kB 24.3MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 20kB 6.4MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 30kB 9.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 40kB 6.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 51kB 7.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 61kB 5.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from pretrainedmodels) (1.4.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (from pretrainedmodels) (0.5.0)\n",
            "Collecting munch\n",
            "  Downloading https://files.pythonhosted.org/packages/cc/ab/85d8da5c9a45e072301beb37ad7f833cd344e04c817d97e0cc75681d248f/munch-2.5.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pretrainedmodels) (4.28.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision->pretrainedmodels) (1.17.5)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision->pretrainedmodels) (6.2.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision->pretrainedmodels) (1.12.0)\n",
            "Building wheels for collected packages: pretrainedmodels\n",
            "  Building wheel for pretrainedmodels (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pretrainedmodels: filename=pretrainedmodels-0.7.4-cp36-none-any.whl size=60962 sha256=4136b04ae6fee30dcfdb120db65336695e1084f28e1afd1612a751be16db99cf\n",
            "  Stored in directory: /root/.cache/pip/wheels/69/df/63/62583c096289713f22db605aa2334de5b591d59861a02c2ecd\n",
            "Successfully built pretrainedmodels\n",
            "Installing collected packages: munch, pretrainedmodels\n",
            "Successfully installed munch-2.5.0 pretrainedmodels-0.7.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ul2eft-xJwom",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1b2effb9-df6b-4bd4-d960-2d456b9bb3d9"
      },
      "source": [
        "!pwd"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/Dataset/classification\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bHDH04r8J3_i",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "2f2d0627-baae-4022-81cb-ced237353ad1"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dataset\t\t      out\t\ttain   validate\n",
            "detect_from_video.py  README.md\t\ttest   xception-b5690688.pth\n",
            "network\t\t      requirements.txt\ttrain\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yat9-UyIO-RS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "3a90f698-0f9f-4ad3-ec8b-5b3a4500c49b"
      },
      "source": [
        "video_path = \"/content/drive/Dataset/classification/train/\"\n",
        "videos = os.listdir(video_path)\n",
        "print(\"Total videos = \"+str(len(videos)))\n",
        "print(video_path + videos[0])\n",
        "\"\"\"\n",
        "for video in videos:\n",
        "  video_path_test = (video_path + video)\n",
        "  print(video_path_test)\n",
        "  \"\"\""
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total videos = 240\n",
            "/content/drive/Dataset/classification/train/a141.mp4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nfor video in videos:\\n  video_path_test = (video_path + video)\\n  print(video_path_test)\\n  '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QEX9LynE5DkK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import argparse\n",
        "from os.path import join\n",
        "\n",
        "\n",
        "import torch\n",
        "import pretrainedmodels\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from network.xception import xception\n",
        "import math\n",
        "import torchvision\n",
        "\n",
        "import cv2\n",
        "import dlib\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "from PIL import Image as pil_image\n",
        "from tqdm import tqdm\n",
        "from torchsummary import summary\n",
        "from dataset.transform import xception_default_data_transforms\n",
        "\n",
        "from torchvision import models"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CRvB0-fncFSU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "33dd2851-0d62-40df-ee35-cf353f13b86f"
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oxhV9YIXE_3t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def return_pytorch04_xception(pretrained=True):\n",
        "    # Raises warning \"src not broadcastable to dst\" but thats fine\n",
        "    model = xception(pretrained=False)\n",
        "    if pretrained:\n",
        "        # Load model in torch 0.4+\n",
        "        model.fc = model.last_linear\n",
        "        del model.last_linear\n",
        "        state_dict = torch.load(F\"/content/drive/Dataset/classification/xception-b5690688.pth\")\n",
        "        for name, weights in state_dict.items():\n",
        "            if 'pointwise' in name:\n",
        "                state_dict[name] = weights.unsqueeze(-1).unsqueeze(-1)\n",
        "        model.load_state_dict(state_dict)\n",
        "        model.last_linear = model.fc\n",
        "        del model.fc\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ubZXJOvGFN3T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TransferModel(nn.Module):\n",
        "    \"\"\"\n",
        "    Simple transfer learning model that takes an imagenet pretrained model with\n",
        "    a fc layer as base model and retrains a new fc layer for num_out_classes\n",
        "    \"\"\"\n",
        "    def __init__(self, modelchoice, num_out_classes=2, dropout=0.0):\n",
        "        super(TransferModel, self).__init__()\n",
        "        self.modelchoice = modelchoice\n",
        "        if modelchoice == 'xception':\n",
        "            self.model = return_pytorch04_xception()\n",
        "            # Replace fc\n",
        "            num_ftrs = self.model.last_linear.in_features\n",
        "            if not dropout:\n",
        "                self.model.last_linear = nn.Linear(num_ftrs, num_out_classes)\n",
        "            else:\n",
        "                print('Using dropout', dropout)\n",
        "                self.model.last_linear = nn.Sequential(\n",
        "                    nn.Dropout(p=dropout),\n",
        "                    nn.Linear(num_ftrs, num_out_classes)\n",
        "                )\n",
        "        elif modelchoice == 'resnet50' or modelchoice == 'resnet18':\n",
        "            if modelchoice == 'resnet50':\n",
        "                self.model = models.resnet50(pretrained=True)\n",
        "            if modelchoice == 'resnet18':\n",
        "                self.model = models.resnet18(pretrained=True)\n",
        "            # Replace fc\n",
        "            num_ftrs = self.model.fc.in_features\n",
        "            if not dropout:\n",
        "                self.model.fc = nn.Linear(num_ftrs, num_out_classes)\n",
        "            else:\n",
        "                self.model.fc = nn.Sequential(\n",
        "                    nn.Dropout(p=dropout),\n",
        "                    nn.Linear(num_ftrs, num_out_classes)\n",
        "                )\n",
        "        else:\n",
        "            raise Exception('Choose valid model, e.g. resnet50')\n",
        "\n",
        "    def set_trainable_up_to(self, boolean, layername=\"Conv2d_4a_3x3\"):\n",
        "        \"\"\"\n",
        "        Freezes all layers below a specific layer and sets the following layers\n",
        "        to true if boolean else only the fully connected final layer\n",
        "        :param boolean:\n",
        "        :param layername: depends on network, for inception e.g. Conv2d_4a_3x3\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        # Stage-1: freeze all the layers\n",
        "        if layername is None:\n",
        "            for i, param in self.model.named_parameters():\n",
        "                param.requires_grad = True\n",
        "                return\n",
        "        else:\n",
        "            for i, param in self.model.named_parameters():\n",
        "                param.requires_grad = False\n",
        "        if boolean:\n",
        "            # Make all layers following the layername layer trainable\n",
        "            ct = []\n",
        "            found = False\n",
        "            for name, child in self.model.named_children():\n",
        "                if layername in ct:\n",
        "                    found = True\n",
        "                    for params in child.parameters():\n",
        "                        params.requires_grad = True\n",
        "                ct.append(name)\n",
        "            if not found:\n",
        "                raise Exception('Layer not found, cant finetune!'.format(\n",
        "                    layername))\n",
        "        else:\n",
        "            if self.modelchoice == 'xception':\n",
        "                # Make fc trainable\n",
        "                for param in self.model.last_linear.parameters():\n",
        "                    param.requires_grad = True\n",
        "\n",
        "            else:\n",
        "                # Make fc trainable\n",
        "                for param in self.model.fc.parameters():\n",
        "                    param.requires_grad = True\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.model(x)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQaqHhtXFRWb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model_selection(modelname, num_out_classes,\n",
        "                    dropout=None):\n",
        "    \"\"\"\n",
        "    :param modelname:\n",
        "    :return: model, image size, pretraining<yes/no>, input_list\n",
        "    \"\"\"\n",
        "    if modelname == 'xception':\n",
        "        return TransferModel(modelchoice='xception',\n",
        "                             num_out_classes=num_out_classes), 299, \\\n",
        "               True, ['image'], None\n",
        "    elif modelname == 'resnet18':\n",
        "        return TransferModel(modelchoice='resnet18', dropout=dropout,\n",
        "                             num_out_classes=num_out_classes), \\\n",
        "               224, True, ['image'], None\n",
        "    else:\n",
        "        raise NotImplementedError(modelname)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_f_UILFCgMZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_boundingbox(face, width, height, scale=1.3, minsize=None):\n",
        "    \"\"\"\n",
        "    Expects a dlib face to generate a quadratic bounding box.\n",
        "    :param face: dlib face class\n",
        "    :param width: frame width\n",
        "    :param height: frame height\n",
        "    :param scale: bounding box size multiplier to get a bigger face region\n",
        "    :param minsize: set minimum bounding box size\n",
        "    :return: x, y, bounding_box_size in opencv form\n",
        "    \"\"\"\n",
        "    x1 = face.left()\n",
        "    y1 = face.top()\n",
        "    x2 = face.right()\n",
        "    y2 = face.bottom()\n",
        "    size_bb = int(max(x2 - x1, y2 - y1) * scale)\n",
        "    if minsize:\n",
        "        if size_bb < minsize:\n",
        "            size_bb = minsize\n",
        "    center_x, center_y = (x1 + x2) // 2, (y1 + y2) // 2\n",
        "\n",
        "    # Check for out of bounds, x-y top left corner\n",
        "    x1 = max(int(center_x - size_bb // 2), 0)\n",
        "    y1 = max(int(center_y - size_bb // 2), 0)\n",
        "    # Check for too big bb size for given x, y\n",
        "    size_bb = min(width - x1, size_bb)\n",
        "    size_bb = min(height - y1, size_bb)\n",
        "\n",
        "    return x1, y1, size_bb"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_dhRt60CikD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess_image(image, cuda=True):\n",
        "    \"\"\"\n",
        "    Preprocesses the image such that it can be fed into our network.\n",
        "    During this process we envoke PIL to cast it into a PIL image.\n",
        "    :param image: numpy image in opencv form (i.e., BGR and of shape\n",
        "    :return: pytorch tensor of shape [1, 3, image_size, image_size], not\n",
        "    necessarily casted to cuda\n",
        "    \"\"\"\n",
        "    # Revert from BGR\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    # Preprocess using the preprocessing function used during training and\n",
        "    # casting it to PIL image\n",
        "    preprocess = xception_default_data_transforms['test']\n",
        "    preprocessed_image = preprocess(pil_image.fromarray(image))\n",
        "    # Add first dimension as the network expects a batch\n",
        "    preprocessed_image = preprocessed_image.unsqueeze(0)\n",
        "    if cuda:\n",
        "        preprocessed_image = preprocessed_image.cuda()\n",
        "    return preprocessed_image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56DhCdvXCzgL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict_with_model(image, model, post_function=nn.Softmax(dim=1),cuda=True):\n",
        "    \"\"\"\n",
        "    Predicts the label of an input image. Preprocesses the input image and\n",
        "    casts it to cuda if required\n",
        "    :param image: numpy image\n",
        "    :param model: torch model with linear layer at the end\n",
        "    :param post_function: e.g., softmax\n",
        "    :param cuda: enables cuda, must be the same parameter as the model\n",
        "    :return: prediction (1 = fake, 0 = real)\n",
        "    \"\"\"\n",
        "    # Preprocess\n",
        "    preprocessed_image = preprocess_image(image, cuda)\n",
        "\n",
        "    # Model prediction\n",
        "    output = model(preprocessed_image)\n",
        "    output = post_function(output)\n",
        "\n",
        "    # Cast to desired\n",
        "    _, prediction = torch.max(output, 1)    # argmax\n",
        "    prediction = float(prediction.cpu().numpy())\n",
        "\n",
        "    return int(prediction), output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3KkM3smrC3ha",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_full_image_network(video_path, output_path,start_frame=0, end_frame=None, cuda=True):\n",
        "    \"\"\"\n",
        "    Reads a video and evaluates a subset of frames with the a detection network\n",
        "    that takes in a full frame. Outputs are only given if a face is present\n",
        "    and the face is highlighted using dlib.\n",
        "    :param video_path: path to video file\n",
        "    :param model_path: path to model file (should expect the full sized image)\n",
        "    :param output_path: path where the output video is stored\n",
        "    :param start_frame: first frame to evaluate\n",
        "    :param end_frame: last frame to evaluate\n",
        "    :param cuda: enable cuda\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    print('Starting: {}'.format(video_path))\n",
        "\n",
        "    # Read and write\n",
        "    reader = cv2.VideoCapture(video_path)\n",
        "\n",
        "    video_fn = video_path.split('/')[-1].split('.')[0]+'.avi'\n",
        "    os.makedirs(output_path, exist_ok=True)\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'MJPG')\n",
        "    fps = reader.get(cv2.CAP_PROP_FPS)\n",
        "    num_frames = int(reader.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    writer = None\n",
        "\n",
        "    # Face detector\n",
        "    face_detector = dlib.get_frontal_face_detector()\n",
        "\n",
        "    # Load model\n",
        "    model, *_ = model_selection(modelname='xception', num_out_classes=2)\n",
        "    print(\"Model : \")\n",
        "    print(model)\n",
        "    model = model.cuda()\n",
        "    input_s = (3, image_size, image_size)\n",
        "    print(summary(model, input_s))\n",
        "    \n",
        "    \n",
        "\n",
        "\n",
        "    # Text variables\n",
        "    font_face = cv2.FONT_HERSHEY_SIMPLEX\n",
        "    thickness = 2\n",
        "    font_scale = 1\n",
        "\n",
        "    # Frame numbers and length of output video\n",
        "    frame_num = 0\n",
        "    assert start_frame < num_frames - 1\n",
        "    end_frame = end_frame if end_frame else num_frames\n",
        "    pbar = tqdm(total=end_frame-start_frame)\n",
        "\n",
        "    while reader.isOpened():\n",
        "        _, image = reader.read()\n",
        "        if image is None:\n",
        "            break\n",
        "        frame_num += 1\n",
        "\n",
        "        if frame_num < start_frame:\n",
        "            continue\n",
        "        pbar.update(1)\n",
        "\n",
        "        # Image size\n",
        "        height, width = image.shape[:2]\n",
        "\n",
        "        # Init output writer\n",
        "        if writer is None:\n",
        "            writer = cv2.VideoWriter(join(output_path, video_fn), fourcc, fps,(height, width)[::-1])\n",
        "\n",
        "        # 2. Detect with dlib\n",
        "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "        faces = face_detector(gray, 1)\n",
        "        if len(faces):\n",
        "            # For now only take biggest face\n",
        "            face = faces[0]\n",
        "\n",
        "            # --- Prediction ---------------------------------------------------\n",
        "            # Face crop with dlib and bounding box scale enlargement\n",
        "            x, y, size = get_boundingbox(face, width, height)\n",
        "            cropped_face = image[y:y+size, x:x+size]\n",
        "\n",
        "            # Actual prediction using our model\n",
        "            prediction, output = predict_with_model(cropped_face, model,cuda=cuda)\n",
        "            # ------------------------------------------------------------------\n",
        "\n",
        "            # Text and bb\n",
        "            x = face.left()\n",
        "            y = face.top()\n",
        "            w = face.right() - x\n",
        "            h = face.bottom() - y\n",
        "            label = 'fake' if prediction == 1 else 'real'\n",
        "            color = (0, 255, 0) if prediction == 0 else (0, 0, 255)\n",
        "            output_list = ['{0:.2f}'.format(float(x)) for x in\n",
        "                           output.detach().cpu().numpy()[0]]\n",
        "            cv2.putText(image, str(output_list)+'=>'+label, (x, y+h+30),\n",
        "                        font_face, font_scale,\n",
        "                        color, thickness, 2)\n",
        "            # draw box over face\n",
        "            cv2.rectangle(image, (x, y), (x + w, y + h), color, 2)\n",
        "\n",
        "        if frame_num >= end_frame:\n",
        "            break\n",
        "\n",
        "        # Show\n",
        "        #cv2.imshow('test', image)\n",
        "        cv2_imshow(image)\n",
        "        cv2.waitKey(33)     # About 30 fps\n",
        "        writer.write(image)\n",
        "    pbar.close()\n",
        "    if writer is not None:\n",
        "        writer.release()\n",
        "        print('Finished! Output saved under {}'.format(output_path))\n",
        "    else:\n",
        "        print('Input video file was empty')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kuSC4Bi5C_11",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if __name__ == '__main__':\n",
        "  model, image_size, *_ = model_selection('resnet18', num_out_classes=2)\n",
        "  print(\"Model : Resenet18\")\n",
        "  print(model)\n",
        "  model = model.cuda()\n",
        "  input_s = (3, image_size, image_size)\n",
        "  print(summary(model, input_s))\n",
        "  video_path = \"/content/drive/Dataset/classification/train/\"\n",
        "  #model_path = \"/content/drive/Dataset/classification/xception-b5690688.pth\"\n",
        "  output_path = \"/content/drive/Dataset/classification/tain/out/\"\n",
        "  videos = os.listdir(video_path)\n",
        "  video_path_test = (video_path + videos[0])\n",
        "  test_full_image_network(video_path_test,output_path)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}